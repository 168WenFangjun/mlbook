# TODOs



- Add the term RSS to linear regression, penalized regression 
- Add a "math note" directive (bayesian regression, loss minimization section)
- Discuss ARIMA in C2? 
- Math Appendix and Probability Appendix. Maybe derive *Math Results* here. 
- Change losses to fancy L
- Model set-up over structure? 
- Don't assume known variance for Bayesian reg? 
  - check the lambda tau thing
- log likelihood log-likelihood
- scikit learn vs. sklearn
- Turn binary and multi-class logreg into two sections
- Nonparametric regression w kernels
- Underline vs. headers 
- Added bias term to $\mathbf{x}_n$. 
- Add a section on the datasets in the appendix
- Gradient ascent vs. descent!!
- Something in intro saying like "the overarching purpose of this book is to demonstrate how common ML algorithms can be fit from scratch"
- Multi-class vs. mutliclass
- Math conventions (vectors always row vectors, bold stuff, etc.)
- Mentioning logistic regression in GLM chapter more clearly 
- Nonparametric methods 
- In Generative classifiers, maybe change labels to prior and likelihood (rather than p(y) and p(x|y))
- In Generative classifiers, discuss(/graph?) differences in LDA/QDA areas
- Get rid of __init__ stuff
- Reference data in appendix 
- 







