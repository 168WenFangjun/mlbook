# Common Methods

$$
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
$$

This section will review two methods that are used to fit a variety of machine learning models: *gradient descent* and *cross validation*. These methods will be used repeatedly throughout this book. 



## Gradient Descent

Almost all the models discussed in this book aim to find a set of parameters that minimize a chosen loss function. Sometimes we can find the optimal parameters by taking the derivative of the loss function, setting it equal to 0, and solving. In situations for which no closed-form solution is available, however, we might turn to gradient descent. **Gradient descent** is an iterative approach to approximating the parameters that minimize a differentiable loss function. 

### The Set-Up

Let's first introduce a typical set-up for gradient descent. Suppose we have $N$ observations where each observation has predictors $\bx_n$ and target variable $y_n$. We decide to approximate $y_n$ with $\hat{y}_n = f(\bx_n, \bbetahat)$, where $f()$ is some differentiable function and $\bbetahat$ is a set of parameter estimates. Next, we introduce a differentiable loss function $\mathcal{L}$. For simplicity, let's assume we can write the model's entire loss as the sum of the individual losses across observations. That is, 


$$
\mathcal{L} = \sumN g(y_n, \hat{y}_n),
$$


where $g()$ is some differentiable function penalizing the difference between $y_n$ and $\hat{y}_n$. 

To fit this generic model, we want to find the values of $\bbetahat$ that minimize $\mathcal{L}$. We will likely start with the following derivative:


$$
\begin{align}
\dadb{\mathcal{L}}{\bbetahat} &= \sumN\dadb{g(y_n, \hat{y}_n)}{\bbetahat} \\
&= \sumN\dadb{g(y_n, \hat{y}_n)}{\hat{y}_n}\cdot\dadb{\hat{y}_n}{\bbetahat}. \\
\end{align}
$$


Ideally, we can set the above derivative equal to 0 and solve for $\bbetahat$, giving our optimal solution. If this isn't possible, we can iteratively search for the values of $\bbetahat$ that minimize $\mathcal{L}$. This is the process of gradient descent. 



### An Intuitive Introduction



![gd](/content/appendix/gd.jpg)



To understand this process intuitively, consider the image above showing a model's loss as a function of one parameter, $\beta$. Presumably we can't directly find the value of $\beta$ to minimize $\mathcal{L}$, so we might start by randomly picking a value of $\beta$. Suppose we start at point $A$. From point $A$ we ask "would the loss function decrease if I increased or decreased $\beta$". To answer this question, we calculate the derivative of $\mathcal{L}$ with respect to $\beta$ evaluated at $\beta = A$. Since this derivative is negative, we know that increasing $\beta$  some small amount will decrease the loss! 

Now we know we want to increase $\beta$, but how much? Intuitively, the more negative the derivative, the more the loss will decrease with an increase in $\beta$. So, let's increase $\beta$ proportionally to the negative of the derivative. Letting $\delta$ be the derivative and $\eta$ be some small constant, we might increase $\beta$ with 


$$
\beta \gets \beta - \eta\delta. 
$$


The more negative $\delta$ is, the more we increase $\beta$. 

Now suppose we make the increase and wind up with $\beta = B$. Calculating the derivative again, we get a slightly positive number. This tells us that we went too far: increasing $\beta$ will increase $\mathcal{L}$. However, since the derivative is only *slightly* positive, we want to only make a slight correction. Let's again use the same adjustment, $\beta \gets \beta - \eta\delta$. Since $\delta$ is now slightly positive, $\beta$ will now decrease slightly. We will repeat this same process a fixed number of times or until $\beta$ barely changes. And that is gradient descent!

### The Steps

We can describe gradient descent more concretely with the following steps. Note here that $\bbetahat$ can be a vector, rather than just a single parameter.

1. Choose a small learning rate $\eta$ 

2. Randomly instantiate $\bbetahat$ 

3. For a fixed number of iterations or until some stopping rule is reached:
   1. Calculate $\boldsymbol{\delta} = \partial \mathcal{L}/\partial \bbetahat$

   2. Adjust $\bbetahat$ with
      $$
      \bbetahat \gets \bbetahat - \eta \boldsymbol{\delta}.
      $$



A potential stopping rule might be a minimum change in the magnitude of $\bbetahat$ or a minimum decrease in the loss function $\mathcal{L}$.



### An Example

As a simple example of gradient descent in action, let's derive the ordinary least squares (OLS) regression estimates. (This problem does have a closed-form solution, but we'll use gradient descent to demonstrate the approach). As discussed in {doc}`Chapter 1 </content/c1/concept>`, linear regression models $\hat{y}_n$ with 


$$
\hat{y}_n = \bphi_n^\top \bbetahat,
$$


where $\bphi_n$ is a vector of predictors appended with a leading 1 and $\bbetahat$ is a vector of coefficients. The OLS loss function is defined with 


$$
\mathcal{L}(\bbetahat) = \frac{1}{2}\sumN(y_n - \hat{y}_n)^2 = \frac{1}{2}\sumN (y_n - \bphi^\top_n \bbetahat)^2.
$$


Let's review the process of minimizing this loss function with gradient descent. After choosing $\eta$ and randomly instantiating $\bbetahat$, we iteratively calculate the loss function's gradient:


$$
\boldsymbol{\delta} = \dadb{\mathcal{L}(\bbetahat)}{\bbetahat} =  -\sumN(y_n - \bphi^\top_n \bbetahat)\cdot\bphi_n^\top,
$$


and adjust with 


$$
\bbetahat \gets \bbetahat - \eta\boldsymbol{\delta}.
$$


This is accomplished with the following code. Note that we can also calculate $\boldsymbol{\delta} = -\bPhi^\top(\by - \hat{\by})$, where $\bPhi$ is the {doc}`feature matrix </content/conventions_notation>`, $\by$ is the vector of targets, and $\hat{\by}$ is the vector of fitted values. 



```python
import numpy as np

def OLS_GD(X, y, eta = 1e-3, n_iter = 1e4, add_intercept = True):
  
  ## Add Intercept
  if add_intercept:
    ones = np.ones(X.shape[0]).reshape(-1, 1)
    X = np.concatenate((ones, X), 1)
    
  ## Instantiate
  beta_hat = np.random.randn(X.shape[1])
  
  ## Iterate
  for i in range(int(n_iter)):
    
    ## Calculate Derivative
    yhat = X @ beta_hat
    delta = -X.T @ (y - yhat)
    beta_hat -= delta*eta
    
```



## Cross Validation