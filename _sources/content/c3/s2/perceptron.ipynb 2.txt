{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "# import data\n",
    "wine = datasets.load_wine()\n",
    "X = wine['data']\n",
    "y = wine['target']\n",
    "X_binary = X[y != 2].copy()\n",
    "y_binary = y[y != 2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the perceptron, let's define a few helper functions. The `sign` function returns `1` for positive numbers and `-1` for negative numbers, which will be useful since the perceptron classifies according to \n",
    "\n",
    "$$\n",
    "\\text{sign}(\\bbeta^\\top \\bx_n).\n",
    "$$\n",
    "\n",
    "The `convert_y` function takes a vector $\\mathbf{y} \\in \\{0, 1\\}$ and turns 0's into -1's such that $\\mathbf{y} \\in \\{-1, +1\\}$. Finally, the `standard_scaler` standardizes, similar to `sklearn`'s `StandardScaler`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that we don't actually need to use the `sign` function. Instead, we could deem an observation correctly classified if $y_n \\hat{y}_n \\geq 0$ and misclassified otherwise. We use it here to be consistent with the derivation in the content section.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(a):\n",
    "    return (-1)**(a < 0)\n",
    "\n",
    "def convert_y(y):\n",
    "    if set(y) == {0, 1}:\n",
    "        return 2*y - 1\n",
    "    else:\n",
    "        raise ValueError('Input is not binary')\n",
    "\n",
    "def standard_scaler(X):\n",
    "    mean = X.mean(0)\n",
    "    sd = X.std(0)\n",
    "    return (X - mean)/sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is implemented below. As usual, we optionally standardize and add an intercept term. Then we fit $\\bbetahat$ with the algorithm introduced in the {doc}`concept section </content/c3/s2/perceptron>`. \n",
    "\n",
    "This implementation tracks whether the perceptron has converged (i.e. all training algorithms are fitted correctly) and stops fitting if so. If not, it will run until `n_iters` is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, n_iter = 10**3, lr = 0.001, add_intercept = True, standardize = True):\n",
    "        \n",
    "        # Add Info #\n",
    "        if standardize:\n",
    "            X = standard_scaler(X)\n",
    "        if add_intercept:\n",
    "            ones = np.ones(len(X)).reshape(-1, 1)\n",
    "        self.X = X\n",
    "        self.N, self.D = self.X.shape\n",
    "        self.y = y\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.converged = False\n",
    "        \n",
    "        # Fit #\n",
    "        beta = np.random.randn(self.D)/5\n",
    "        for i in range(self.n_iter):\n",
    "            misclassifications = False\n",
    "            for n in range(self.N):\n",
    "                yhat_n = sign(np.dot(beta, self.X[n]))\n",
    "                if (self.y[n]*yhat_n == -1):\n",
    "                    beta += self.lr * self.y[n]*self.X[n]\n",
    "                    misclassifications = True\n",
    "            if misclassifications == False:\n",
    "                self.converged = True\n",
    "                self.iterations_until_convergence = i\n",
    "                break\n",
    "                \n",
    "        # Return Values #\n",
    "        self.beta = beta\n",
    "        self.yhat = sign(np.dot(self.X, self.beta))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model. We'll again use the `wine` dataset from `sklearn.datasets` and remove the third class so the task is binary. We can also check whether the perceptron converged and, if so, after how many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_y_binary = convert_y(y_binary)\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_binary, converted_y_binary, lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 4 iterations\n"
     ]
    }
   ],
   "source": [
    "if perceptron.converged:\n",
    "    print(f\"Converged after {perceptron.iterations_until_convergence} iterations\")\n",
    "else:\n",
    "    print(\"Not converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(perceptron.yhat == perceptron.y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
