$$
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
$$

# Conventions and Notation



The following terminology will be used throughout the book.  

- Variables can be split into two types: the variables we intend to model are referred to as **target** or **output** variables, while the variables we use to model the target variables are referred to as **predictors**, **features**, or **input** variables. These are also known as the *dependent* and *independent* variables, respectively.
- An **observation** is a single collection of predictors and target variables. Multiple observations with the same variables are combined to form a **dataset**. 
- A **training** dataset is one used to build a machine learning model. A **validation** dataset is one used to compare multiple models built on the same training dataset with different parameters. A **testing** dataset is one used to evaluate a final model. 
- Variables, whether predictors or targets, may be **quantitative** or **categorical**. Quantitative variables follow a continuous or near-contih234nuous scale (such as height in inches or income in dollars). Categorical variables fall in one of a discrete set of groups (such as nation of birth or species type). While the values of categorical variables may follow some natural order (such as shirt size), this is not assumed. 
- Modeling tasks are referred to as **regression** if the target is quantitative and **classification** if the target is categorical. Note that regression does not necessarily refer to ordinary least squares (OLS) linear regression. 



Unless indicated otherwise, the following conventions are used to represent data and datasets. 

- Training datasets are assumed to have $N$ observations and $D$ predictors. When appropriate (as indicated in the content sections), this will include a column of 1s to represent the intercept term.  

- The vector of features for the $n^\text{th}$ observation is given by $\bx_n$. Note that $\bx_n$ might include functions of the original predictors through feature engineering. When the target variable is single-dimensional (i.e. there is only one target variable per observation), it is given by $y_n$; when there are multiple target variables per observation, the vector of targets is given by $\by_n$.

- Many models, such as ordinary linear regression, append an intercept term to the predictor vector. For clarity, the predictor vector will be indicated by $\bphi_n$ when this is done. That is, for a vector of features
  
  $$
  \bx_n = \begin{pmatrix} x_{n,1} & x_{n,2} & ... & x_{n,D-1} \end{pmatrix},
  $$

  the vector $\bphi_n$ is given by 

  $$
  \bphi_n = \begin{pmatrix} 1 & x_{n,1} & x_{n,2} &  ... & x_{n, D-1}\end{pmatrix}. 
  $$
  

- *Feature matrices* or *data frames* are created by concatenating feature vectors across observations. Within a matrix, feature vectors are row vectors, with $\bx_n$ or $\bphi_n$ representing the matrix's $n^\text{th}$ row. These matrices are then given by $\bX$ or $\bPhi$, depending on whether an intercept term is included or not. Within a feature matrix $\bPhi$, the first column will consist of only 1s. 

  

Finally, the following mathematical and notational conventions are used.

- Scalar values will be non-boldface and lowercase, vectors will be bold and lowercase, and matrices will be bold and uppercase. E.g. $b$ is a scalar, $\mathbf{b}$ a vector, and $\mathbf{B}$ a matrix. 

- Unless indicated otherwise, all vectors are assumed to be column vectors. Since feature vectors (such as $\bx_n$ and $\bphi_n$ above) are entered into data frames as rows, they will sometimes be treated as row vectors, even outside of data frames.

- Matrix or vector derivatives, covered in the {doc}`math appendix </content/appendix/math>`, will use the numerator [layout convention](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions). Under this convention, the derivative $\partial\by/\partial\bx$ is written as 
  

  $$
  \dadb{\by}{\bx} = \begin{pmatrix}
  \dadb{y_1}{x_1} & ... & \dadb{y_1}{x_b} \\
  \dadb{y_2}{x_1} &  ... & \dadb{y_2}{x_b} \\
  &  ... & \\
  \dadb{y_a}{x_1} & ... & \dadb{y_a}{x_b} \\
  \end{pmatrix} ,
  $$
  


  for $\by \in \R^a$ and $\bx \in \R^b$. 

