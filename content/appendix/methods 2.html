

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Common Methods &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}})</script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Datasets" href="data.html" />
    <link rel="prev" title="Probability" href="probability.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../table_of_contents.html">Table of Contents</a>
  </li>
  <li class="">
    <a href="../conventions_notation.html">Conventions and Notation</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">1. Ordinary Linear Regression</p>
</li>
  <li class="">
    <a href="../c1/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c1/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c1/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">2. Linear Regression Extensions</p>
</li>
  <li class="">
    <a href="../c2/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c2/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c2/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">3. Discriminative Classifiers (Logistic Regression)</p>
</li>
  <li class="">
    <a href="../c3/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c3/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c3/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">4. Generative Classifiers (Naive Bayes)</p>
</li>
  <li class="">
    <a href="../c4/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c4/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c4/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">5. Decision Trees</p>
</li>
  <li class="">
    <a href="../c5/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c5/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c5/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">6. Tree Ensemble Methods</p>
</li>
  <li class="">
    <a href="../c6/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c6/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c6/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">7. Neural Networks</p>
</li>
  <li class="">
    <a href="../c7/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c7/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c7/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Appendix</p>
</li>
  <li class="">
    <a href="math.html">Math</a>
  </li>
  <li class="">
    <a href="probability.html">Probability</a>
  </li>
  <li class="active">
    <a href="">Common Methods</a>
  </li>
  <li class="">
    <a href="data.html">Datasets</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../../_sources/content/appendix/methods.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        <a class="edit-button" href="https://github.com/dafriedman97/book/edit/master/content/appendix/methods.md"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" title="Edit this page"><i class="fas fa-pencil-alt"></i></button></a>

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#gradient-descent" class="nav-link">1. Gradient Descent</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#the-set-up" class="nav-link">The Set-Up</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#an-intuitive-introduction" class="nav-link">An Intuitive Introduction</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#the-steps" class="nav-link">The Steps</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#an-example" class="nav-link">An Example</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#cross-validation" class="nav-link">2. Cross Validation</a>
        </li>
    
    </ul>
</nav>



<div class="tocsection editthispage">
    <a href="https://github.com/dafriedman97/book/edit/master/content/appendix/methods.md">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="common-methods">
<h1>Common Methods<a class="headerlink" href="#common-methods" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\]</div>
<p>This section will review two methods that are used to fit a variety of machine learning models: <em>gradient descent</em> and <em>cross validation</em>. These methods will be used repeatedly throughout this book.</p>
<div class="section" id="gradient-descent">
<h2>1. Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Almost all the models discussed in this book aim to find a set of parameters that minimize a chosen loss function. Sometimes we can find the optimal parameters by taking the derivative of the loss function, setting it equal to 0, and solving. In situations for which no closed-form solution is available, however, we might turn to gradient descent. <strong>Gradient descent</strong> is an iterative approach to approximating the parameters that minimize a differentiable loss function.</p>
<div class="section" id="the-set-up">
<h3>The Set-Up<a class="headerlink" href="#the-set-up" title="Permalink to this headline">¶</a></h3>
<p>Let’s first introduce a typical set-up for gradient descent. Suppose we have <span class="math notranslate nohighlight">\(N\)</span> observations where each observation has predictors <span class="math notranslate nohighlight">\(\bx_n\)</span> and target variable <span class="math notranslate nohighlight">\(y_n\)</span>. We decide to approximate <span class="math notranslate nohighlight">\(y_n\)</span> with <span class="math notranslate nohighlight">\(\hat{y}_n = f(\bx_n, \bbetahat)\)</span>, where <span class="math notranslate nohighlight">\(f()\)</span> is some differentiable function and <span class="math notranslate nohighlight">\(\bbetahat\)</span> is a set of parameter estimates. Next, we introduce a differentiable loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>. For simplicity, let’s assume we can write the model’s entire loss as the sum of the individual losses across observations. That is,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = \sumN g(y_n, \hat{y}_n),
\]</div>
<p>where <span class="math notranslate nohighlight">\(g()\)</span> is some differentiable function penalizing the difference between <span class="math notranslate nohighlight">\(y_n\)</span> and <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>.</p>
<p>To fit this generic model, we want to find the values of <span class="math notranslate nohighlight">\(\bbetahat\)</span> that minimize <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>. We will likely start with the following derivative:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\dadb{\mathcal{L}}{\bbetahat} &amp;= \sumN\dadb{g(y_n, \hat{y}_n)}{\bbetahat} \\
&amp;= \sumN\dadb{g(y_n, \hat{y}_n)}{\hat{y}_n}\cdot\dadb{\hat{y}_n}{\bbetahat}. \\
\end{align}
\end{split}\]</div>
<p>Ideally, we can set the above derivative equal to 0 and solve for <span class="math notranslate nohighlight">\(\bbetahat\)</span>, giving our optimal solution. If this isn’t possible, we can iteratively search for the values of <span class="math notranslate nohighlight">\(\bbetahat\)</span> that minimize <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>. This is the process of gradient descent.</p>
</div>
<div class="section" id="an-intuitive-introduction">
<h3>An Intuitive Introduction<a class="headerlink" href="#an-intuitive-introduction" title="Permalink to this headline">¶</a></h3>
<p><img alt="gd" src="../../_images/gd.jpg" /></p>
<p>To understand this process intuitively, consider the image above showing a model’s loss as a function of one parameter, <span class="math notranslate nohighlight">\(\beta\)</span>. Presumably we can’t directly find the value of <span class="math notranslate nohighlight">\(\beta\)</span> to minimize <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>, so we might start by randomly picking a value of <span class="math notranslate nohighlight">\(\beta\)</span>. Suppose we start at point <span class="math notranslate nohighlight">\(A\)</span>. From point <span class="math notranslate nohighlight">\(A\)</span> we ask “would the loss function decrease if I increased or decreased <span class="math notranslate nohighlight">\(\beta\)</span>”. To answer this question, we calculate the derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\beta\)</span> evaluated at <span class="math notranslate nohighlight">\(\beta = A\)</span>. Since this derivative is negative, we know that increasing <span class="math notranslate nohighlight">\(\beta\)</span>  some small amount will decrease the loss!</p>
<p>Now we know we want to increase <span class="math notranslate nohighlight">\(\beta\)</span>, but how much? Intuitively, the more negative the derivative, the more the loss will decrease with an increase in <span class="math notranslate nohighlight">\(\beta\)</span>. So, let’s increase <span class="math notranslate nohighlight">\(\beta\)</span> proportionally to the negative of the derivative. Letting <span class="math notranslate nohighlight">\(\delta\)</span> be the derivative and <span class="math notranslate nohighlight">\(\eta\)</span> be some small constant, we might increase <span class="math notranslate nohighlight">\(\beta\)</span> with</p>
<div class="math notranslate nohighlight">
\[
\beta \gets \beta - \eta\delta. 
\]</div>
<p>The more negative <span class="math notranslate nohighlight">\(\delta\)</span> is, the more we increase <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>Now suppose we make the increase and wind up with <span class="math notranslate nohighlight">\(\beta = B\)</span>. Calculating the derivative again, we get a slightly positive number. This tells us that we went too far: increasing <span class="math notranslate nohighlight">\(\beta\)</span> will increase <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>. However, since the derivative is only <em>slightly</em> positive, we want to only make a slight correction. Let’s again use the same adjustment, <span class="math notranslate nohighlight">\(\beta \gets \beta - \eta\delta\)</span>. Since <span class="math notranslate nohighlight">\(\delta\)</span> is now slightly positive, <span class="math notranslate nohighlight">\(\beta\)</span> will now decrease slightly. We will repeat this same process a fixed number of times or until <span class="math notranslate nohighlight">\(\beta\)</span> barely changes. And that is gradient descent!</p>
</div>
<div class="section" id="the-steps">
<h3>The Steps<a class="headerlink" href="#the-steps" title="Permalink to this headline">¶</a></h3>
<p>We can describe gradient descent more concretely with the following steps. Note here that <span class="math notranslate nohighlight">\(\bbetahat\)</span> can be a vector, rather than just a single parameter.</p>
<ol>
<li><p>Choose a small learning rate <span class="math notranslate nohighlight">\(\eta\)</span></p></li>
<li><p>Randomly instantiate <span class="math notranslate nohighlight">\(\bbetahat\)</span></p></li>
<li><p>For a fixed number of iterations or until some stopping rule is reached:</p>
<ol>
<li><p>Calculate <span class="math notranslate nohighlight">\(\boldsymbol{\delta} = \partial \mathcal{L}/\partial \bbetahat\)</span></p></li>
<li><p>Adjust <span class="math notranslate nohighlight">\(\bbetahat\)</span> with</p>
<div class="math notranslate nohighlight">
\[
      \bbetahat \gets \bbetahat - \eta \boldsymbol{\delta}.
      \]</div>
</li>
</ol>
</li>
</ol>
<p>A potential stopping rule might be a minimum change in the magnitude of <span class="math notranslate nohighlight">\(\bbetahat\)</span> or a minimum decrease in the loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
</div>
<div class="section" id="an-example">
<h3>An Example<a class="headerlink" href="#an-example" title="Permalink to this headline">¶</a></h3>
<p>As a simple example of gradient descent in action, let’s derive the ordinary least squares (OLS) regression estimates. (This problem does have a closed-form solution, but we’ll use gradient descent to demonstrate the approach). As discussed in <a class="reference internal" href="../c1/concept.html"><span class="doc">Chapter 1</span></a>, linear regression models <span class="math notranslate nohighlight">\(\hat{y}_n\)</span> with</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_n = \bphi_n^\top \bbetahat,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bphi_n\)</span> is a vector of predictors appended with a leading 1 and <span class="math notranslate nohighlight">\(\bbetahat\)</span> is a vector of coefficients. The OLS loss function is defined with</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\bbetahat) = \frac{1}{2}\sumN(y_n - \hat{y}_n)^2 = \frac{1}{2}\sumN (y_n - \bphi^\top_n \bbetahat)^2.
\]</div>
<p>Let’s review the process of minimizing this loss function with gradient descent. After choosing <span class="math notranslate nohighlight">\(\eta\)</span> and randomly instantiating <span class="math notranslate nohighlight">\(\bbetahat\)</span>, we iteratively calculate the loss function’s gradient:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\delta} = \dadb{\mathcal{L}(\bbetahat)}{\bbetahat} =  -\sumN(y_n - \bphi^\top_n \bbetahat)\cdot\bphi_n^\top,
\]</div>
<p>and adjust with</p>
<div class="math notranslate nohighlight">
\[
\bbetahat \gets \bbetahat - \eta\boldsymbol{\delta}.
\]</div>
<p>This is accomplished with the following code. Note that we can also calculate <span class="math notranslate nohighlight">\(\boldsymbol{\delta} = -\bPhi^\top(\by - \hat{\by})\)</span>, where <span class="math notranslate nohighlight">\(\bPhi\)</span> is the <a class="reference internal" href="../conventions_notation.html"><span class="doc">feature matrix</span></a>, <span class="math notranslate nohighlight">\(\by\)</span> is the vector of targets, and <span class="math notranslate nohighlight">\(\hat{\by}\)</span> is the vector of fitted values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">OLS_GD</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">,</span> <span class="n">add_intercept</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
  
  <span class="c1">## Add Intercept</span>
  <span class="k">if</span> <span class="n">add_intercept</span><span class="p">:</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
  <span class="c1">## Instantiate</span>
  <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  
  <span class="c1">## Iterate</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)):</span>
    
    <span class="c1">## Calculate Derivative</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="n">beta_hat</span> <span class="o">-=</span> <span class="n">delta</span><span class="o">*</span><span class="n">eta</span>
    
</pre></div>
</div>
</div>
</div>
<div class="section" id="cross-validation">
<h2>2. Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h2>
<p>Several of the models covered in this book require <em>hyperparameters</em> to be chosen exogenously—i.e. before the model is fit. The value of these hyperparameters affects the quality of the fit. So how can we choose these values without fitting a model? The most common answer is cross validation.</p>
<p>Suppose we are deciding between several values of a hyperparameter, resulting in multiple competing models. One way to choose our model would be to split our data into a <em>training</em> set and a <em>validation</em> set, build each model on the training set, and see which performs better on the validation set. By splitting the data into training and validation, we avoid evaluating a model based on its in-sample performance.</p>
<p>The obvious problem with this set-up is that we are comparing the performance of models on just <em>one</em> dataset. Instead, we might choose between competing models with <strong>K-fold cross validation</strong>, outlined below.</p>
<ol class="simple">
<li><p>Split the original dataset into <span class="math notranslate nohighlight">\(K\)</span> <em>folds</em> or subsets.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>, treat fold <span class="math notranslate nohighlight">\(k\)</span> as the validation set. Train each competing model on the other <span class="math notranslate nohighlight">\(K-1\)</span> folds and evaluate it on the <span class="math notranslate nohighlight">\(k^\text{th}\)</span>.</p></li>
<li><p>Select the model with the best average validation performance.</p></li>
</ol>
<p>As an example, let’s use cross validation to choose a penalty value for a <a class="reference internal" href="../c2/s1/regularized.html"><span class="doc">Ridge regression</span></a> model, discussed in chapter 2. This model constrains the magnitude of the regression coefficients; the higher the penalty term, the more the coefficients are constrained.</p>
<p>The example below uses the <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> class from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, which defines the penalty term with the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> argument. We will use the <a class="reference internal" href="data.html"><span class="doc">boston housing</span></a> dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Import packages </span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>

<span class="c1">## Import data</span>
<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1">## Choose alphas to consider</span>
<span class="n">potential_alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">error_by_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">potential_alphas</span><span class="p">))</span>

<span class="c1">## Choose the folds </span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

<span class="c1">## Iterate through folds</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
  
  <span class="c1">## Split Train and Validation</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">folds</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">folds</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">folds</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
    <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">folds</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
  
  <span class="c1">## Iterate through Alphas</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">potential_alphas</span><span class="p">)):</span>
    
        <span class="c1">## Train on Training Set</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">potential_alphas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1">## Calculate and Append Error</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">y_val</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
        <span class="n">error_by_alpha</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">error</span>
    
<span class="n">error_by_alpha</span> <span class="o">/=</span> <span class="n">N</span>
</pre></div>
</div>
<p>We can then check <code class="docutils literal notranslate"><span class="pre">error_by_alpha</span></code> and choose the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> corresponding to the lowest average error!</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="probability.html" title="previous page">Probability</a>
    <a class='right-next' id="next-link" href="data.html" title="next page">Datasets</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>