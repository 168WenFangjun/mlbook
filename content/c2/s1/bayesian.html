

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Bayesian Regression &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/mystnb.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}})</script>
    <link rel="shortcut icon" href="../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="GLMs" href="GLMs.html" />
    <link rel="prev" title="Regularized Regression" href="regularized.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../table_of_contents.html">Table of Contents</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">1. Ordinary Linear Regression</p>
</li>
  <li class="">
    <a href="../../c1/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c1/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c1/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">2. Linear Regression Extensions</p>
</li>
  <li class="active">
    <a href="../concept.html">Concept</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="regularized.html">Regularized Regression</a>
    </li>
    <li class="active">
      <a href="">Bayesian Regression</a>
    </li>
    <li class="">
      <a href="GLMs.html">GLMs</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">3. Discriminative Classifiers (Logistic Regression)</p>
</li>
  <li class="">
    <a href="../../c3/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c3/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c3/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">4. Generative Classifiers (Naive Bayes)</p>
</li>
  <li class="">
    <a href="../../c4/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c4/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c4/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">5. Decision Trees</p>
</li>
  <li class="">
    <a href="../../c5/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c5/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c5/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">6. Tree Ensemble Methods</p>
</li>
  <li class="">
    <a href="../../c6/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c6/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c6/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">7. Neural Networks</p>
</li>
  <li class="">
    <a href="../../c7/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c7/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c7/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">8. Unsupervised Learning</p>
</li>
  <li class="">
    <a href="../../c8/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c8/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c8/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Common ML Methods</p>
</li>
  <li class="">
    <a href="../../c9/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c9/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c9/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Appendix</p>
</li>
  <li class="">
    <a href="../../appendix/math.html">Math</a>
  </li>
  <li class="">
    <a href="../../appendix/probability.html">Probability</a>
  </li>
  <li class="">
    <a href="../../appendix/data.html">Data</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../../../_sources/content/c2/s1/bayesian.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        <a class="edit-button" href="https://github.com/dafriedman97/book/edit/master/content/c2/s1/bayesian.md"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" title="Edit this page"><i class="fas fa-pencil-alt"></i></button></a>

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-bayesian-set-up" class="nav-link">The Bayesian Set-Up</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-results" class="nav-link">The Results</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#intuition" class="nav-link">Intuition</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#full-results" class="nav-link">Full Results</a>
        </li>
    
            </ul>
        </li>
    
    </ul>
</nav>



<div class="tocsection editthispage">
    <a href="https://github.com/dafriedman97/book/edit/master/content/c2/s1/bayesian.md">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayesian-regression">
<h1>Bayesian Regression<a class="headerlink" href="#bayesian-regression" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\]</div>
<p>In the Bayesian approach to statistical inference, we treat our parameters as random variables and assign them a prior distribution. This forces our estimates to reconcile our existing beliefs about these parameters with new information given by the data. This approach can be applied to linear regression by assigning the regression coefficients a prior distribution.</p>
<p>We also may wish to perform Bayesian regression not because of a prior belief about the coefficients but in order to minimize model complexity. By assigning the parameters a prior distribution with mean 0, we force the posterior estimates to be closer to 0 than they would otherwise. This is a form of regularization similar to the ridge and lasso methods discussed in the <a class="reference internal" href="regularized.html"><span class="doc">previous section</span></a>.</p>
<div class="section" id="the-bayesian-set-up">
<h2>The Bayesian Set-Up<a class="headerlink" href="#the-bayesian-set-up" title="Permalink to this headline">¶</a></h2>
<p>To demonstrate Bayesian regression, we’ll follow three typical steps to Bayesian analysis: writing the likelihood, writing the prior density, and using Bayes’ Rule to get the posterior density. In the <a class="reference internal" href="#results"><span class="std std-ref">results</span></a> below, we use the posterior density to calculate the maximum-a-posteriori (MAP).</p>
<p><u>1. The Likelihood</u></p>
<p>As in the typical regression set-up, let’s assume</p>
<div class="math notranslate nohighlight">
\[
y_n \iid \mathcal{N}\left(\bbeta^\top \bx_n + \epsilon_n, \sigma^2\right).
\]</div>
<p>We can write the collection of observations jointly as</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\by &amp;\sim \mathcal{N}\left( \bX\bbeta, \bSigma\right),
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\by \in \mathbb{R}^N\)</span> and <span class="math notranslate nohighlight">\(\bSigma = \sigma^2 I_N \in \mathbb{R}^{N \times N}\)</span> for some scalar <span class="math notranslate nohighlight">\(\sigma^2\)</span>. For simplicity, let’s assume <span class="math notranslate nohighlight">\(\sigma^2\)</span> is known.</p>
<div class="alert alert-info">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://www.statlect.com/fundamentals-of-statistics/Bayesian-regression">this lecture</a> for an example of Bayesian regression without the assumption of known variance.</p>
</div>
<p>We can then get our likelihood and log likelihood using the Multivariate Normal.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L(\bbeta; \bX, \by) 
&amp;= 
\frac{1}{\sqrt{(2\pi)^N|\bSigma|}}\exp\left(-\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) \right) 
\\
&amp;\propto \exp\left(-\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) \right) 
\\
\log L(\bbeta; \bX, \by) &amp;= -\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta).
\end{align*}
\end{split}\]</div>
<p><br></br></p>
<p><u>2. The Prior</u></p>
<p>Now, let’s assign <span class="math notranslate nohighlight">\(\bbeta\)</span> a prior distribution. We typically assume</p>
<div class="math notranslate nohighlight">
\[
\bbeta \sim \mathcal{N}(\mathbf{0}, \bT),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bbeta \in \mathbb{R}^D\)</span> and <span class="math notranslate nohighlight">\(\bT = \tau I_D \in \mathbb{R}^{D \times D}\)</span> for some scalar <span class="math notranslate nohighlight">\(\tau\)</span>. We choose <span class="math notranslate nohighlight">\(\tau\)</span> (and therefore <span class="math notranslate nohighlight">\(\bT\)</span>) ourselves, with a greater <span class="math notranslate nohighlight">\(\tau\)</span> giving less weight to the prior.</p>
<p>The prior density is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\bbeta) &amp;= 
\frac{1}{\sqrt{(2\pi)^D|\bT|}}\exp\left(-\frac{1}{2}\bbeta^\top\bT^{-1}\bbeta \right) 
\\
&amp;\propto \exp\left(-\frac{1}{2}\bbeta^\top\bT^{-1}\bbeta \right)
\\
\log p(\bbeta) &amp;= -\frac{1}{2}\bbeta^\top \bT^{-1}\bbeta.
\end{align*}
\end{split}\]</div>
<p><br></br></p>
<p><u>3. The Posterior</u></p>
<p>We are then interested in a posterior density of <span class="math notranslate nohighlight">\(\bbeta\)</span> given the data, <span class="math notranslate nohighlight">\(\bX\)</span> and <span class="math notranslate nohighlight">\(\by\)</span>. Note that this is more or less the Bayesian equivalent to the <span class="math notranslate nohighlight">\(\bbetahat\)</span> estimate from the typical (Frequentist) approach.</p>
<p>Bayes’ rule tells us that the posterior density is proportional to the likelihood times the prior density. Using the two previous results, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\bbeta|\bX, \by) &amp;\propto L(\bbeta; \bX, \by) p(\bbeta) 
\\
\log p(\bbeta|\bX, \by) &amp;= \log L(\bbeta; \bX, \by) + \log p(\bbeta) + k
\\
&amp;=  -\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) - \frac{1}{2}\bbeta^\top \bT^{-1}\bbeta + k 
\\
&amp;= -\frac{1}{2\sigma^2}(\by - \bX\bbeta)^\top(\by - \bX\bbeta) - \frac{1}{2\tau}\bbeta^\top \bbeta + k 
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is some constant that we don’t care about.</p>
</div>
<div class="section" id="the-results">
<span id="results"></span><h2>The Results<a class="headerlink" href="#the-results" title="Permalink to this headline">¶</a></h2>
<div class="section" id="intuition">
<h3>Intuition<a class="headerlink" href="#intuition" title="Permalink to this headline">¶</a></h3>
<p>Often in the Bayesian setting it is infeasible to obtain the entire posterior distribution. Instead, one typically looks at the maximum-a-posteriori (MAP), the value of the parameters that maximize the posterior density. In our case, the MAP is the <span class="math notranslate nohighlight">\(\bbetahat\)</span> that maximizes</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log p(\bbetahat|\bX, \by) &amp;= -\frac{1}{2\sigma^2}(\by - \bX\bbetahat)^\top(\by - \bX\bbetahat) - \frac{1}{2\tau}\bbetahat^\top \bbetahat.
\end{align*}
\]</div>
<p>This is equivalent to finding the <span class="math notranslate nohighlight">\(\bbetahat\)</span> that minimizes the following loss function, where <span class="math notranslate nohighlight">\(\lambda\)</span> is some scalar (specifically <span class="math notranslate nohighlight">\(\frac{1}{\tau}\)</span>).</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
L(\bbetahat) &amp;= \frac{1}{2}(\by - \bX\bbetahat)^\top(\by - \bX\bbetahat) + \frac{\lambda}{2}\bbetahat^\top \bbetahat 
\\
&amp;= \frac{1}{2}(\by - \bX\bbetahat)^\top(\by - \bX\bbetahat) + \frac{\lambda}{2} \sum_{d = 0}^D\hat{\beta}_d.
\end{align}
\end{split}\]</div>
<p>Notice that this is extremely close to the Ridge loss function discussed in the <a class="reference internal" href="regularized.html"><span class="doc">previous section</span></a>—it is not quite equal to the Ridge loss function since it also penalizes the magnitude of the intercept, though this difference could be eliminated by changing prior distribution of the intercept.</p>
<p>This shows that Bayesian regression with a mean-zero Normal prior distribution is essentially equivalent to Ridge regression. Decreasing <span class="math notranslate nohighlight">\(\tau\)</span>, just like decreasing <span class="math notranslate nohighlight">\(\lambda\)</span>, increases the amount of regularization.</p>
</div>
<div class="section" id="full-results">
<h3>Full Results<a class="headerlink" href="#full-results" title="Permalink to this headline">¶</a></h3>
<p>Now let’s actually derive the MAP by calculating the gradient of the log posterior density.</p>
<div class="admonition-math-note alert alert-info">
<p class="admonition-title">Math Note</p>
<p>For a symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \mathbf{s}}\left(\mathbf{q} - \mathbf{A}\mathbf{s} \right)^\top \mathbf{W}\left(\mathbf{q} - \mathbf{A}\mathbf{s}\right) = -2\mathbf{A}^\top \mathbf{W}\left(\mathbf{q} - \mathbf{A}\mathbf{s}\right)
\]</div>
<p>This implies that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \mathbf{s}}\mathbf{s}^\top \mathbf{W}\mathbf{s} = 
\frac{\partial}{\partial \mathbf{s}} (\mathbf{0} - I\mathbf{s})^\top \mathbf{W} (\mathbf{0} - I\mathbf{s})= 
2\mathbf{W}\mathbf{s}.
\]</div>
</div>
<p>Using the <em>Math Note</em> above, we have</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\dadb{}{\bbeta} \log p(\bbeta|\bX, \by) &amp;= \bX^\top \bSigma^{-1}(\by - \bX \bbeta) - \bT^{-1}\bbeta.
\end{align*}
\]</div>
<p>We calculate the MAP by setting this gradient equal to 0:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\bbetahat &amp;= \left(\bX^\top\bSigma^{-1} \bX + \bT^{-1}\right)^{-1}\bX^\top\bSigma^{-1}\by \\
&amp;= \left(\frac{1}{\sigma^2}\bX^\top\bX + \frac{1}{\tau} I_D\right)^{-1}\frac{1}{\sigma^2}\bX^\top\by.
\end{align*}
\end{split}\]</div>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="regularized.html" title="previous page">Regularized Regression</a>
    <a class='right-next' id="next-link" href="GLMs.html" title="next page">GLMs</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
  </body>
</html>